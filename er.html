<html>
<head>
  <style>
  body {
    font-family: sans-serif;
    color: rgb(79,79,79);
    margin: 0 auto;
    max-width: 50%;
    line-height: 1.75;
    padding: 4em 1em;
    background-color: rgb(239, 245, 255);
  }
  .image {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
h1 {
  font-family: 'Aresenal', sans-serif;
  color: rgb(69,105,144);
}
h2 {
  font-family: 'Muli', sans-serif;
  color: rgb(17,75,95);
}
a {
  color: rgb(2,128,144);
  text-decoration: none;
}
a:hover {
  transition: all 0.3s;
  color: rgb(244,91,105);
  text-decoration: none;
}
  @import url('https://fonts.googleapis.com/css?family=Muli');
  @import url('https://fonts.googleapis.com/css?family=Arsenal:');

  </style>
</head>
<body>
<h1>Ethical Reflection 1<h1>
  <h2>The Impact of Allowing Algorithms Make Life-Changing Decisions</h2>
  <img
  src="https://images.unsplash.com/photo-1461749280684-dccba630e2f6?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=e5a31d03ddee66863a599421f792e07b&auto=format&fit=crop&w=500&q=60" class="image">
</img>
  <p> <strong>By: Laura Ospina</strong> </p>
  <p>
  Our world runs on algorithms. We revolve on what they tell us to do, what they do for us, and the information they provide for our daily wants and needs. Computers are presented as an omniscient power, unriddled by human bias and mistakes. People tend to trust them more, relying and not questioning the consequences of their disinterest. Algorithms decide on life-changing decisions such as who is called for a job interview, allowed parole, sentenced for crimes longer, and other influential occurrences. The government and even the companies who provide this information have little to no interest in regulating or analyzing the algorithms, breeding bias. Their inactiveness when regarding the bias make many algorithms noxious to those affected.
</p>
<p>
	A risk score was initially designed to provide accurate probation and treatment programs to defendants who would benefit from them. The founder of Northpointe’s COMPAS assessment, Tim Bernann, testified this purpose as well. Ranging in low to high risk to the community, the algorithm tests if a person convicted of a crime or misdemeanor would commit another infraction in the future. However, courtrooms around the United States are using it in every step of criminal litigation, affecting the sentencing the most.
</p>
<p>
	The bias is obvious in places such as Broward County, Florida, where African-Americans are 77% more likely to be labeled as a high risk of violent crimes. The 2013-2014 study showed that black people who were labeled higher risk but didn’t reoffend was 44.9% and for the same statistic for white people, 23.5%. When the data is accumulated by ProPublica, it isn’t hard to see that African-Americans with a short list of petty misdemeanors were labeled as high risk to commit a violent crime whereas a white counterpart with a much stronger list of felonies might be labeled as medium or low risk.
</p>
<p>
The algorithm is shown time and time again to have specific bias against minorities or low-income individuals. Still, some judges depend on it for sentencing a defendant. A black man whose sentencing should have been six months to one year in county jail, a judge admits, received two years of state prison because he relied on the risk score. Another black woman, having never been arrested and yet labeled medium risk, is unable to find a job at McDonald’s.
</p>
<p>
We are supported by algorithms everyday but should we trust them so blindly? In this case, they are affecting lives forever, preventing others from getting jobs, keeping them incarcerated for long periods of time, limiting the help they need and deserve. The sentences inhibit people from supporting their families, leaving them struggling into the next generation, and the cycle of poverty ensues. Misusing and relying on algorithms isn’t the way to deal with human beings if they are blatantly targeting minorities. Purely basing decisions on the shoulders of humans who in danger of bias and mistake shouldn’t be the sole way. We
</p>
<p>
  Instead, as the digital age grows, we must be conscious of the bias algorithms have and the effect they have on us and the lives of others. We must be ready to call out for monitoring and improvements from the corporations who are more worried about the bottom line than those in their line of fire. In the meantime, the risk scores and other such algorithm based assessments should be open to everyone in court, the data especially, permitting the score to not directly influence sentencing. As a collective society, advocating for the refining of algorithms is paramount. Technology’s goal is to make our lives easier, not deprive due process and equal treatment, particularly in our justice system. When letting computers and algorithms decide life-changing circumstances, let us know they chose the right ones.
</p>
<p>
<strong>Sources:</strong>
</p>
<p>
<a href="https://www.technologyreview.com/s/608248/biased-algorithms-are-everywhere-and-no-one-seems-to-care/">
Biased Algorithms are Everywhere and No One Seems to Care by Will Knight</a>
</p>
<p>
<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">
Machine Bias Risk Assessments in Criminal Sentencing by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner</a>
</p>
</body>
</html>
