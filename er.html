<html>
<head>
  <style>
  body {
    font-family: sans-serif;
    color: rgb(79,79,79);
    margin: 0 auto;
    max-width: 50%;
    line-height: 1.75;
    padding: 4em 1em;
  }
h1 {
  font-family: 'Aresenal', sans-serif;
  color: rgb(69,105,144);
}
h2 {
  font-family: 'Muli', sans-serif;
  color: rgb(17,75,95)
}
a {
  color: rgb(2,128,144)
}
a:hover {
  transition: all .3s
  color: rgb(244,91,105)
}
  @import url('https://fonts.googleapis.com/css?family=Muli');
  @import url('https://fonts.googleapis.com/css?family=Arsenal:');
  <
  </style>
</head>
<body>
<h1>Ethical Reflection 1<h1>
  <h2>When should we let computers rule life-changing decisions?</h2>
  <p> <strong>By: Laura Ospina</strong> </p>
  <p>
  Our world runs on algorithms. We revolve on the what they tell us to do, what they do for us, and the information they provide, for our daily wants and needs. Computers are presented as an omniscient power, unriddled by human bias and mistakes. People tend to trust them more, relying and not questioning the consequences of their disinterest. Algorithms decide on life-changing decisions such as who is called for a job interview, allowed parole, sentenced for crimes longer, and other influential occurrences. The government and even the companies who provide this information have little to no interest in regulating or analyzing the algorithms. Their inactiveness when regarding the bias many algorithms have are noxious to those affected.
</p>
<p>
	A risk score was initially designed to provide accurate probation and treatment programs to defendants who would benefit from them. The founder of Northpointe’s COMPAS assessment, Tim Bernann, testified this purpose as well. Ranging in low to high risk to the community, the algorithm tests if a person convicted of a crime or misdemeanor would in the future commit another infraction. However, courtrooms around the United States are using it in every step of criminal litigation, affecting the sentencing the most.
</p>
<p>
	The bias is obvious in places such as Broward County, Florida, where African-Americans are 77% more likely to be labeled as a high risk of violent crimes. The 2013-2014 study showed that black people who were labeled higher risk but didn’t reoffend was 44.9% and for the same statistic for white people, 23.5%. When the data is accumulated by ProPublica, it isn’t hard to see that African-Americans with a short list of petty misdemeanors were labeled as high risk to commit a violent crime whereas a white counterpart with a much stronger list of felonies might be labeled as medium or low risk.
</p>
<p>
The algorithm is shown time and time again to have specific bias against minorities or low-income individuals. Still, some judges depend on it for sentencing a defendant. A black man whose sentencing should have been six months to one year in county jail, a judge admits, received two years of state prison because he relied on the risk score. Another black woman, having never been arrested and yet labeled medium risk, is unable to find a job at McDonald’s.
</p>
<p>
We are supported by algorithms everyday but should we trust them so blindly? In this case, they are affecting lives forever, preventing them from getting jobs, keeping them incarcerated for long periods of time, limiting the help they need and deserve. The sentences inhibit people from supporting their families, leaving them struggling into the next generation, and the cycle of poverty ensues. Misusing and relying on algorithms isn’t the way to deal with human beings if they are blatantly targeting minorities. Purely basing decisions on the shoulders of humans who in danger of bias and mistake shouldn’t be the sole way.
</p>
<p>
<strong>Sources:</strong>
</p>
<p>
<a href="https://www.technologyreview.com/s/608248/biased-algorithms-are-everywhere-and-no-one-seems-to-care/">
Biased Algorithms are Everywhere and No One Seems to Care by Will Knight</a>
</p>
<p>
<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">
Machine Bias Risk Assessments in Criminal Sentencing by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner</a>
</p>
</body>
</html>
